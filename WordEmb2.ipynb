{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordEmb2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5kLnG0ELvhX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bd9b48e3-76ed-46f6-b887-3ddd14194d9b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxJxz-GWL2te",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "a47a93fa-555e-47f7-c075-6e6cb4b72289"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import h5py\n",
        "from gensim.models import Word2Vec\n",
        "import gensim.models\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from keras.layers import Dense, Dropout, Activation, Bidirectional\n",
        "from keras.layers import LSTM, SimpleRNN, GRU\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "#import preprocess\n",
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc_bFoFzj7v5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "01d5b854-52c0-45e0-d9c7-a61534a84125"
      },
      "source": [
        "## import word2vec with google\n",
        "WORD2VEC_VECTORS_BIN  =  \"/content/gdrive/My Drive/SFResearch/Clickbait_Datasets/GoogleNews-vectors-negative300.bin.gz\"\n",
        "\n",
        "w2v  =  gensim . models . KeyedVectors . load_word2vec_format ( WORD2VEC_VECTORS_BIN , binary = True )\n",
        "stop_words  =  set ( stopwords . words ( 'english' ))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-57228ef7329b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mWORD2VEC_VECTORS_BIN\u001b[0m  \u001b[0;34m=\u001b[0m  \u001b[0;34m\"/content/gdrive/My Drive/SFResearch/Clickbait_Datasets/GoogleNews-vectors-negative300.bin.gz\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mw2v\u001b[0m  \u001b[0;34m=\u001b[0m  \u001b[0mgensim\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0mKeyedVectors\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0mload_word2vec_format\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mWORD2VEC_VECTORS_BIN\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mstop_words\u001b[0m  \u001b[0;34m=\u001b[0m  \u001b[0mset\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mstopwords\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;34m'english'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1436\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_not_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    461\u001b[0m                 \u001b[0;31m# jump to the next member, if there is one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_gzip_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36m_read_gzip_header\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_gzip_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mmagic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmagic\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprepend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 5] Input/output error"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLc6el0okJYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Data cleaning\n",
        "df [ 'cleaned_tweet' ] =  df [ 'text' ]. apply ( preprocess . clean_tweet )\n",
        "\n",
        "## to find the maximum number of words in a sentence in order to determine the sequence size\n",
        "L = []\n",
        "for  i , token  in  enumerate ( df [ 'cleaned_tweet' ]):\n",
        "    word = [ w  for  w  in  token . split () if  not  w  in  stop_words ]\n",
        "    L . append ( len ( word ))\n",
        "    \n",
        "sequence_size = max ( L )\n",
        "\n",
        "## separate data non-randomly to avoid having a problem during cross validation due to indexes\n",
        "\n",
        "split = int ( 7 * len ( df ) / 8 )\n",
        "\n",
        "text_train , text_test , y_train , y_test = df . iloc [: split , 2 ], df . ILOC [ split :, 2 ], df . iloc [: split , 1 ], df . ILOC [ split :, 1 ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USwfmAnNkToG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "2c0bd872-6373-47a4-8480-c995d56022e4"
      },
      "source": [
        "## apply the word2 with google but WORD by WORD !! the problem is that not all the words will freeze in the word2vec\n",
        "\n",
        "dimsize = 300\n",
        "def  compute_matrix ( text ):\n",
        "    X = np . zeros (( len ( text ), sequence_size , dimsize ))\n",
        "\n",
        "\n",
        "    for  i , token  in  enumerate ( text ):\n",
        "        word = token . split ()\n",
        "        try :\n",
        "        \n",
        "            j = 0\n",
        "            for  w  in  word :\n",
        "                if  w  not  in  stop_words :\n",
        "                    X [ i , j ] = w2v [ w ]\n",
        "                    j + = 1\n",
        "        except :\n",
        "            pass\n",
        "    return  X \n",
        "\n",
        "X_train = compute_matrix ( text_train )\n",
        "X_test = compute_matrix ( text_test )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-8beb4a0e5757>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    j + = 1\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9wQJiVfkZRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## create the model \n",
        "def  creat_model_RNN ():\n",
        "    \n",
        "    model  =  Sequential ()\n",
        "    model . add ( Bidirectional ( SimpleRNN ( 128 , dropout = 0.1 ), input_shape = ( sequence_size , dimsize )))   # try using a GRU and a SimpleRNN\n",
        "    model . add ( Dense ( 1 ))\n",
        "    model . add ( Activation ( 'sigmoid' ))\n",
        "    model . compile ( loss = 'binary_crossentropy' , optimizer = 'rmsprop' , metrics = [ 'accuracy' ])\n",
        "    \n",
        "    return  model\n",
        "\n",
        "def  creat_model_GRU ():\n",
        "    \n",
        "    model  =  Sequential ()\n",
        "    model . add ( Bidirectional ( GRU ( 128 , dropout = 0.1 ), input_shape = ( sequence_size , dimsize )))   # try using a GRU and a SimpleRNN\n",
        "    model . add ( Dense ( 1 ))\n",
        "    model . add ( Activation ( 'sigmoid' ))\n",
        "    model . compile ( loss = 'binary_crossentropy' , optimizer = 'rmsprop' , metrics = [ 'accuracy' ])\n",
        "    \n",
        "    return  model\n",
        "\n",
        "def  creat_model_LSTM ():\n",
        "    \n",
        "    model  =  Sequential ()\n",
        "    model . add ( Bidirectional ( LSTM ( 128 , dropout = 0.1 ), input_shape = ( sequence_size , dimsize )))   # try using a GRU and a SimpleRNN\n",
        "    model . add ( Dense ( 1 ))\n",
        "    model . add ( Activation ( 'sigmoid' ))\n",
        "    model . compile ( loss = 'binary_crossentropy' , optimizer = 'rmsprop' , metrics = [ 'accuracy' ])\n",
        "    \n",
        "    return  model\n",
        "\n",
        "## define a file that saves the weights of the best model\n",
        "cp = ModelCheckpoint ( \"best_model.h5\" , verbose = 1 , save_best_only = True )\n",
        "earlystop_cb  =  keras . callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 12 , verbose = 1 , mode = 'auto' )\n",
        " \n",
        "callback_list = [ cp , earlystop_cb ]  \n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "def  fit_model ( model , X_train , y_train , x_valid , y_valid , batch_size  =  batch_size ):\n",
        "\n",
        "    #earlystop_cb = keras.callbacks.EarlyStopping (monitor = 'val_loss', patience = 7, verbose = 1, mode = 'auto')\n",
        "\n",
        "    model . fit ( X_train , y_train , batch_size = batch_size , epochs = 20 , validation_data = [ x_valid , y_valid ], callbacks = callback_list )\n",
        "    #score, acc = model.evaluate (X_test, y_test, batch_size = batch_size)\n",
        "    \n",
        "    return  model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7V6lUzpSkhei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## cross_validation 10folds\n",
        "batch_size = 64\n",
        "n_folds  =  10\n",
        "skf  =  StratifiedKFold ( n_splits = n_folds , shuffle = True )\n",
        "\n",
        "\n",
        "for  i , ( train , valid ) in  enumerate ( skf . split ( X_train , y_train )):\n",
        "    print ( X_train [ train ]. shape , y_train [ train ]. shape , X_train [ valid ]. shape , y_train [ valid ]. shape )\n",
        "    print ( \"Running Fold\" , i + 1 , \"/\" , n_folds )\n",
        "    model  =  None  # Clearing the NN.\n",
        "    model  =  creat_model_RNN ()\n",
        "    hist = fit_model ( model , X_train [ train ], y_train [ train ], X_train [ valid ], y_train [ valid ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gu0kDvQHkmCd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## you can plot the loss function for train and valid per epoch\n",
        "loss_train = hist . history [ 'loss' ]\n",
        "loss_val = hist . history [ 'val_loss' ]\n",
        "plt . plot ( loss_train , \"b\" , label = \"loss_train\" )\n",
        "plt . plot ( loss_val , \"r\" , label = \"loss_valid\" )\n",
        "plt . title ( \"loss over training epochs\" )\n",
        "plt . legend ()\n",
        "plt . show ()    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6hidqxekrPp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the weights of the best model to use them in the test\n",
        "hist . load_weights ( \"best_model.h5\" )\n",
        "\n",
        "##EVALUATION\n",
        "score , acc  =  hist . evaluate ( X_test , y_test ,\n",
        "                            batch_size = batch_size )\n",
        "print ( 'Test score:' , score )\n",
        "print ( 'Test accuracy:' , acc )\n",
        "#prediction\n",
        "y_pred  =  model . predict_classes ( X_test )\n",
        "y_scores  =  model . predict_proba ( X_test )\n",
        "\n",
        "## display prediction results\n",
        "roc  =  roc_auc_score ( y_test , y_scores )\n",
        "print ( 'ROC score:' , roc )\n",
        "\n",
        "metrics  =  classification_report ( y_test , y_pred , digits = 4 )\n",
        "print ( 'Classification Report \\ n ' )\n",
        "print ( metrics )\n",
        "\n",
        "cm  =  confusion_matrix ( y_test , y_pred )\n",
        "print ( 'Confusion Matrix \\ n ' )\n",
        "print ( cm )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}